{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3d1b6e-ef16-4c2e-bddc-00f161d01271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Logo + Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f842a34-2d4b-486a-bac3-b091741a6107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/stmball/.local/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /home/stmball/.local/lib/python3.10/site-packages (0.18.1)\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Requirement already satisfied: filelock in /home/stmball/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/stmball/.local/lib/python3.10/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/stmball/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/stmball/.local/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/stmball/.local/lib/python3.10/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/stmball/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: numpy in /home/stmball/.local/lib/python3.10/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/stmball/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/stmball/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49afe0c8-c659-452a-a32f-93797f83e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.io import read_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a97cd-85bc-4ba3-a618-898198e070ce",
   "metadata": {},
   "source": [
    "# Welcome to the $NAME AI Challenge!\n",
    "\n",
    "We've all heard about artificial intelligence (AI) changing the world, but did you know it's making waves in how we care for our eyes? Today, we're going to explore some amazing ways that AI is helping doctors keep our vision sharp and healthy. \n",
    "\n",
    "AI is all around us, being used for everything from unlocking how drugs interact with our bodies to trying to defend against cyberattacks to make the internet safer; but how does it work?\n",
    "\n",
    "In our session today, we'll take a look at:\n",
    "\n",
    "1. What is AI, and how do modern AI models work?\n",
    "2. How can we train our own AI to detect eye disease?\n",
    "3. What are some of the considerations we need to consider when using this model?\n",
    "\n",
    "So with that said, let's get stuck in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b357465-a021-4d5a-bff4-312fcb028d02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# What is AI, and how does it work?\n",
    "\n",
    "AI models are computer programs that aim to perform complex tasks to save us time and effort. Sometimes this is about performing something tedious for us, but fairly difficult for simple computer programs (for example, saying if an image is of a \"cat\" or a \"dog\"), but sometimes it allows us to analyse huge datasets in ways we previously couldn't (for example, detecting fraudulant credit card transactions). \n",
    "\n",
    "Most cutting-edge AI works using *neural networks*; these are massive mathematical models originally developed to imitate how our brains worked! These models work by having millions (or  billions now!) of *parameters* that change as the model learns, improving the model as training continues.\n",
    "\n",
    "The way a model \"learns\" is by having a *loss function*; this is a general measure of how \"well\" the model is doing. The higher the loss, the worse the model performs. Sometimes this loss is obvious (e.g. - did the model correctly guess \"cat\" for cat images?), but for more advanced models loss functions can get quite creative.\n",
    "\n",
    "SAM NOTE: Write some more here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3a99c-5d4f-4367-8d48-a0954be1e692",
   "metadata": {},
   "source": [
    "# A brief (!) detour - Python and coding\n",
    "\n",
    "To train AI, we need a way of talking to our computer - and programming languages are the way to go about doing this. By using code, we can give instructions to our computer for it to perform billions of computations during training. \n",
    "\n",
    "One of the most popular coding languages to train AI is Python - which is what we're using in this document. By using Python, we have access to a huge number of AI tools that make training significantly easier. \n",
    "\n",
    "Using these notebooks, we can run code alongside reading text and viewing images. Any time you see a block like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92fd7e-2db5-462c-bdfb-ada5cc442f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054597c-ec2f-47fe-a14c-8fb75ab73256",
   "metadata": {},
   "source": [
    "You can run the cell and any outputs will show below. Try running the cell by clicking it and pressing SHIFT + ENTER on your keyboard!\n",
    "\n",
    "Learning Python is a huge task in itself, but well worth it; if training your own AI interests you in learning more, please let us know and we can signpost you to some good resources for learning more!\n",
    "\n",
    "In this session, we'll code together - we'll explain the bits that are new, and you'll be able to tinker with the code and write your own to make the AI work. Let's start by looking at the above block of code.\n",
    "\n",
    "`print` is a *function* - these are like digital machines that accept inputs (in this case, the text \"Hello world!\"), and do something with it (in this case, show it in the output box below. In fact, \"inside\" the function is actually more Python code that runs in the background, and we can write our own functions to organise our code in an easier way.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "1. Try changing text between the speech marks above and see what happens.\n",
    "2. The `len` function gives us the length of the object we enter in - for text this is the number of letters including spaces. How many letters does `\"The quick brown fox jumps over the lazy dog have\"` (including spaces)?\n",
    "3. `sorted` will sort an object into ascending order (in text's case, this is alphabetical order of letters). However to use this in an interesting way, we need to introduce lists. List are a collection of elements. Here we are *assigning* the list to the variable `a` - this means that whenever Python \"sees\" `a`, it will `think` of the value it is assigned to. Try running the `sorted` function on `a` and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2ae093-769b-4a1a-a5e8-48af841e5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how lists are written!\n",
    "a = [5, 13, 2, 1, 3, 8, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc122a3-39eb-4aca-ace2-76ade54b319b",
   "metadata": {},
   "source": [
    "# Training our own AI\n",
    "\n",
    "Let's now try and train our own AI for eye healthcare - we are going to build a model for detecting Glaucoma from fundus photography.\n",
    "\n",
    "Glaucoma is a common eye condition that is the leading cause of blindness in the UK. Clinicans will take a photo of the back of the eye and examine the images for features that are characteristic to the condition.\n",
    "\n",
    "[EXAMPLE PHOTOS]\n",
    "\n",
    "We have an open-source dataset from the Rotterdam EyePACS AIROGS challenge (https://www.kaggle.com/datasets/deathtrooper/eyepacs-airogs-light?resource=download) that we are going to use to train our AI model. Under the `data/train` directory, we have 2500 positive (RG) and 2500 negative cases (NRG) of glaucoma.\n",
    "\n",
    "Our first job is to create Python object that contains all of our data ready to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e69b166c-2e33-46bb-9a88-3ebe549ceb7b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FundusDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.files[idx]\n",
    "        \n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        label = 0 if \"NRG\" in str(image_path) else 1\n",
    "\n",
    "        return image, labe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b6db79-c23f-40ed-b608-28f4cfc881ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get all the image files in the training folder.\n",
    "# .glob finds files based on a pattern; ** means \"any folder\"\n",
    "# and * means \"any file\" - as long as it ends in .jpg!\n",
    "\n",
    "files = list(Path(\"./data/train\").glob(\"**/*.jpg\"))\n",
    "\n",
    "# We have created the FundusDataset object to convert this list\n",
    "# into a type of object PyTorch needs. It's fairly straightforward,\n",
    "# but the code is a bit challenging, so we've collapsed it above.\n",
    "training_dataset = FundusDataset(files)\n",
    "\n",
    "# Next, we convert this into a \"DataLoader\" - this prepares the \n",
    "# data ready to be put into the AI model!\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b2b37-56f4-4603-a237-060e87135dd4",
   "metadata": {},
   "source": [
    "The next thing we want to do is load and train the model. We are going to be doing this using the ResNet model (https://arxiv.org/abs/1512.03385), a well established, mature network that performs well on classification tasks.\n",
    "\n",
    "We do not need to know exactly how this AI works (although that's the excitement of AI research!), but we only need to know that we need to set up a few things for training to take place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d1015e1-e222-4a9c-ac07-9bae6dcb707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "             ReLU-17           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 32, 32]             256\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 32, 32]             256\n",
      "           Conv2d-24          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 32, 32]             256\n",
      "             ReLU-26          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "             ReLU-30          [-1, 128, 32, 32]               0\n",
      "           Conv2d-31          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-34          [-1, 128, 32, 32]               0\n",
      "           Conv2d-35          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 16, 16]             512\n",
      "             ReLU-37          [-1, 256, 16, 16]               0\n",
      "           Conv2d-38          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 16, 16]             512\n",
      "           Conv2d-40          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 16, 16]             512\n",
      "             ReLU-42          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "             ReLU-46          [-1, 256, 16, 16]               0\n",
      "           Conv2d-47          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 16, 16]             512\n",
      "             ReLU-49          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-50          [-1, 256, 16, 16]               0\n",
      "           Conv2d-51            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-53            [-1, 512, 8, 8]               0\n",
      "           Conv2d-54            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-56            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-58            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "           Conv2d-63            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-65            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-66            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 82.01\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 127.35\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/stmball/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Pre-training with Pytorch; loss function, build model etc\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(3,256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33985999-4d99-41aa-ae2c-85c0397e753a",
   "metadata": {},
   "source": [
    "Now we can get to training our model; this is the part where we write the least code, but the computer does the most work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a08734e-4ba5-456f-8228-63ea96bccfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72556f2-2bdc-461a-a4ba-bd1378a8cfe5",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Now that we have trained the model; we want to test how well the model is doing. When testing our models, it's unfair to use the data the model already has; we don't want to test the *memory* of the model, but the *understanding* of the model to look at the typical features of glaucoma. Typically, we split the dataset to reserve some images specifically for testing for this purpose - in fact, the `data` folder has a specific foldder for us here. All we need to do is set up the images in the same way as training and get the outputs.\n",
    "\n",
    "From there, we can use some measures to see how well our model stacks up. There are plenty of metrics that all measure slightly different things; and later we'll think about the pros and cons of different ones. For now, we'll be using the `f1_score`, which is a fairly standard place to start for classification tasks. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef9b9e3-d07e-47c7-9413-0220bdd82930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: F1_score for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b49397-4841-45b7-a96d-e9a0b689c017",
   "metadata": {},
   "source": [
    "# Thinking about AI Impact\n",
    "\n",
    "If you've got this far, you have successfully trained your own AI to detect eye disease. Well done! Let's think about some considerations when using this model.\n",
    "\n",
    "## Model Validity and Metrics\n",
    "\n",
    "We have measured the F1 score of the model - but this metric doesn't mean much in a vaccum; how good is the F1 score that we got in real terms? Ideally, we'd look at what a current clinical accuracy is to see if we're getting close to reaching expert accuracy, or if we're far off. If we're beating the experts, fantastic! However if we're not as good as clinicians, we can do additional analysis into why that is the case. Maybe we have a small number of cases that are difficult for AI but easy for experts. Maybe our training set isn't representitive of the whole population?\n",
    "\n",
    "Not thinking about these problems is what leads to systematic imbalances in AI, and what drives inequality in AI outcomes. If the dataset is not representitive of the general population (e.g. does not contain BAME samples), then the outputs will have far lower performance for these groups. It's our job as AI researchers to think about these challenges and solve them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5257d9-4e3b-4278-a02e-ade34f24206c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
